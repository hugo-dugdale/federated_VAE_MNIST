{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Activity VAE**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "basedir = os.getcwd()\n",
    "datadir = os.path.join(basedir, 'data')\n",
    "\n",
    "# Training hyperparameters\n",
    "N_ROUNDS = 15\n",
    "N_CENTRES = 4\n",
    "N_EPOCHS = 5\n",
    "N_FEATURES = 19\n",
    "BATCH_SIZE = 2\n",
    "LATENT_DIM = 5\n",
    "\n",
    "# GPU settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class to create pytorch Dataset of activity data\n",
    "class ActivityDataset(Dataset):\n",
    "    \"\"\" Custom PyTorch Dataset \n",
    "        of activity data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_array):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            data_array (array): NumPy array of data.\n",
    "        \"\"\"\n",
    "        self.data_array = data_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_array)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sample = self.data_array[idx]\n",
    "        label = sample[-1]\n",
    "        sample = sample[:-1]\n",
    "        sample = torch.Tensor(sample)\n",
    "\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data points: 25979\n",
      "Test data points: 5081\n",
      "Number of features: 20\n"
     ]
    }
   ],
   "source": [
    "# Load training and test\n",
    "data_file = 'date-val_False.npz'\n",
    "data = np.load(os.path.join(datadir, data_file))\n",
    "train_data = data['X_train']\n",
    "test_data = data['X_test']\n",
    "\n",
    "# Make dataframes\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Print some statisics\n",
    "print(f'Training data points: {train_data.shape[0]}')\n",
    "print(f'Test data points: {test_data.shape[0]}')\n",
    "assert train_data.shape[1] == test_data.shape[1], f'Training and test data have different number of features ({train_data.shape[1]}, {train_data.shape[1]})'\n",
    "print(f'Number of features: {train_data.shape[1]}')\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = ActivityDataset(train_data)\n",
    "test_dataset = ActivityDataset(test_data)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Define VAE model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\" Convulutional VAE model.\"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features=20,\n",
    "                 latent_dim=5):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Assign variables\n",
    "        self.in_features = in_features\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Build Encoder\n",
    "        modules = []\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.in_features, self.in_features // 2),\n",
    "                nn.LeakyReLU()\n",
    "                )\n",
    "        )\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "        # Linear layers defining distribution parameters\n",
    "        self.fc_mu = nn.Linear(self.in_features // 2, self.latent_dim)\n",
    "        self.fc_var = nn.Linear(self.in_features // 2, self.latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.latent_dim, self.in_features //2),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(self.in_features //2, self.in_features),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "        )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "      \n",
    "    def encode(self, x):\n",
    "        \"\"\" Encode input to mean and logvar.\"\"\"\n",
    "        result = self.encoder(x)\n",
    "        mu = self.fc_mu(result)\n",
    "        logvar = self.fc_var(result)\n",
    "\n",
    "        return [mu, logvar]\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        \"\"\" Reparameterise to sample.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        return eps * std + mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\" Decode latent sampling to output.\"\"\"\n",
    "        result = self.decoder(z)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of the network.\"\"\"\n",
    "        # Check input matches in_features\n",
    "        if x.shape[-1] != self.in_features:\n",
    "            raise Exception(f'Input features must be equal to {self.in_features}!')\n",
    "        \n",
    "        # Encode input to mean and logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "\n",
    "        # Reparameterise\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        return [self.decode(z), mu, logvar]\n",
    "\n",
    "# Loss function\n",
    "class VAELoss(nn.Module):\n",
    "    \"\"\" Loss function for VAE\n",
    "        using BCE loss and KL \n",
    "        divergence.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VAELoss, self).__init__()\n",
    "        self.criterion = F.binary_cross_entropy\n",
    "\n",
    "    def forward(self, outputs, inputs, mu, logvar, Beta=5):   \n",
    "        recon_loss = self.criterion(outputs, inputs, reduction='sum')\n",
    "        kl = 0.5 * torch.sum(-1 - logvar + mu.pow(2) + logvar.exp())\n",
    "\n",
    "        return recon_loss, kl, recon_loss + kl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Train Standard VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters for the Standard VAE: 524\n",
      "\n",
      "Standard VAE architecture:\n",
      "VAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=19, out_features=9, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=9, out_features=5, bias=True)\n",
      "  (fc_var): Linear(in_features=9, out_features=5, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=9, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=9, out_features=19, bias=True)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initiliase standard model\n",
    "StandardVAE = VAE(in_features=N_FEATURES, latent_dim=5).to(device)\n",
    "params = sum(p.numel() for p in StandardVAE.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters for the Standard VAE: {}\\n\".format(params))\n",
    "print(\"Standard VAE architecture:\")\n",
    "print(StandardVAE)\n",
    "\n",
    "# Optimiser and loss\n",
    "optimizer = torch.optim.Adam(StandardVAE.parameters(), lr=1e-3)\n",
    "criterion = VAELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Standard VAE training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 12990/12990 [00:12<00:00, 1005.87batch/s, loss=-1.15e+5]\n",
      "Epoch 2: 100%|██████████| 12990/12990 [00:12<00:00, 1029.38batch/s, loss=-1.15e+5]\n",
      "Epoch 3: 100%|██████████| 12990/12990 [00:12<00:00, 1053.96batch/s, loss=-1.4e+5] \n",
      "Epoch 4: 100%|██████████| 12990/12990 [00:12<00:00, 1014.51batch/s, loss=-9.55e+4]\n",
      "Epoch 5: 100%|██████████| 12990/12990 [00:12<00:00, 1010.90batch/s, loss=-9.86e+4]\n",
      "Epoch 6: 100%|██████████| 12990/12990 [00:12<00:00, 1064.28batch/s, loss=9.18e+4] \n",
      "Epoch 7: 100%|██████████| 12990/12990 [00:12<00:00, 1074.86batch/s, loss=-9.62e+4]\n",
      "Epoch 8: 100%|██████████| 12990/12990 [00:12<00:00, 1064.06batch/s, loss=-1.18e+5]\n",
      "Epoch 9: 100%|██████████| 12990/12990 [00:13<00:00, 977.98batch/s, loss=-1.51e+5] \n",
      "Epoch 10: 100%|██████████| 12990/12990 [00:12<00:00, 1028.15batch/s, loss=1.07e+5] \n",
      "Epoch 11: 100%|██████████| 12990/12990 [00:12<00:00, 1023.14batch/s, loss=-88288.5]\n",
      "Epoch 12: 100%|██████████| 12990/12990 [00:12<00:00, 1034.17batch/s, loss=-1.3e+5] \n",
      "Epoch 13: 100%|██████████| 12990/12990 [00:12<00:00, 1016.51batch/s, loss=-1.13e+5]\n",
      "Epoch 14: 100%|██████████| 12990/12990 [00:13<00:00, 987.15batch/s, loss=-1.41e+5] \n",
      "Epoch 15: 100%|██████████| 12990/12990 [00:12<00:00, 1060.95batch/s, loss=-1.42e+5]\n",
      "Epoch 16: 100%|██████████| 12990/12990 [00:12<00:00, 1056.22batch/s, loss=8.78e+4] \n",
      "Epoch 17: 100%|██████████| 12990/12990 [00:12<00:00, 1020.97batch/s, loss=-9.95e+4]\n",
      "Epoch 18: 100%|██████████| 12990/12990 [00:12<00:00, 1014.15batch/s, loss=-9.1e+4] \n",
      "Epoch 19: 100%|██████████| 12990/12990 [00:12<00:00, 1035.20batch/s, loss=2.71e+9] \n",
      "Epoch 20: 100%|██████████| 12990/12990 [00:12<00:00, 1043.93batch/s, loss=-1.51e+5]\n",
      "Epoch 21: 100%|██████████| 12990/12990 [00:12<00:00, 1020.59batch/s, loss=-1.49e+5]\n",
      "Epoch 22: 100%|██████████| 12990/12990 [00:12<00:00, 1017.50batch/s, loss=-1.24e+5]\n",
      "Epoch 23: 100%|██████████| 12990/12990 [00:12<00:00, 1070.51batch/s, loss=-1.02e+5]\n",
      "Epoch 24: 100%|██████████| 12990/12990 [00:11<00:00, 1123.13batch/s, loss=-8.45e+4]\n",
      "Epoch 25: 100%|██████████| 12990/12990 [00:11<00:00, 1110.72batch/s, loss=-1.66e+5]\n"
     ]
    }
   ],
   "source": [
    "# Initialise loss dictionary\n",
    "losses = {'Reconstruction': [],\n",
    "          'KL': [],\n",
    "          'Training': []\n",
    "          }\n",
    "\n",
    "# Begin training\n",
    "print('Beginning Standard VAE training...\\n')\n",
    "StandardVAE.train()\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Running loss containers\n",
    "    running_recon_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    # Loop over batches\n",
    "    with tqdm.tqdm(train_dataloader, unit=\"batch\") as tepoch: \n",
    "        for batch_idx, (data, label) in enumerate(tepoch):\n",
    "          # Batch tensor\n",
    "          batch_tensor = data.to(device)\n",
    "\n",
    "          # Compute reconstructions\n",
    "          results, mu, logvar = StandardVAE(batch_tensor)\n",
    "\n",
    "          # Loss\n",
    "          recon_loss, kl_loss, train_loss = criterion(results, batch_tensor,\n",
    "                                                      mu=mu, logvar=logvar)\n",
    "\n",
    "          # Backpropagation based on the loss\n",
    "          optimizer.zero_grad()\n",
    "          train_loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # Update loss\n",
    "          running_recon_loss += recon_loss.item()\n",
    "          running_kl_loss += kl_loss.item()\n",
    "          running_train_loss += train_loss.item()\n",
    "\n",
    "          # Log\n",
    "          if batch_idx % 20 == 0:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            tepoch.set_postfix(loss=train_loss.item())\n",
    "\n",
    "        # Average epoch loss\n",
    "        losses['Reconstruction'].append(running_recon_loss/batch_idx+1)\n",
    "        losses['KL'].append(running_kl_loss/batch_idx+1)\n",
    "        losses['Training'].append(running_train_loss/batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
